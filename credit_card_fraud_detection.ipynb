{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "credit card fraud detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The challenge is to recognize fraudulent credit card transactions so that the customers of credit card companies are not charged for items that they did not purchase.\n",
        "\n",
        "Main challenges involved in credit card fraud detection are:\n",
        "\n",
        "Enormous Data is processed every day and the model build must be fast enough to respond to the scam in time.\n",
        "Imbalanced Data i.e most of the transactions (99.8%) are not fraudulent which makes it really hard for detecting the fraudulent ones\n",
        "Data availability as the data is mostly private.\n",
        "Misclassified Data can be another major issue, as not every fraudulent transaction is caught and reported.\n",
        "Adaptive techniques used against the model by the scammers.\n",
        "How to tackle these challenges?\n",
        "\n",
        "The model used must be simple and fast enough to detect the anomaly and classify it as a fraudulent transaction as quickly as possible.\n",
        "Imbalance can be dealt with by properly using some methods which we will talk about in the next paragraph\n",
        "For protecting the privacy of the user the dimensionality of the data can be reduced.\n",
        "A more trustworthy source must be taken which double-check the data, at least for training the model.\n",
        "We can make the model simple and interpretable so that when the scammer adapts to it with just some tweaks we can have a new model up and running to deploy."
      ],
      "metadata": {
        "id": "pRqg6ANAw6Vy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Importing all the necessary Libraries"
      ],
      "metadata": {
        "id": "1gT7qR-7w8xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib import gridspec"
      ],
      "metadata": {
        "id": "1idd8VDSw0Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "48BqNJ0Oxg1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Loading the Data"
      ],
      "metadata": {
        "id": "qC7Ugd4DxKBg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"credit.csv\")"
      ],
      "metadata": {
        "id": "YQNhozqgwz9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Understanding the Data"
      ],
      "metadata": {
        "id": "gZDEzduUxkLM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "hKxbYGiaxiMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Describing the Data"
      ],
      "metadata": {
        "id": "uN91bPXqxn_e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(data.shape)\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "po24qR0gxiHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Imbalance in the data\n",
        "Time to explain the data we are dealing with."
      ],
      "metadata": {
        "id": "LbtoKCN3xr0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine number of fraud cases in dataset\n",
        "fraud = data[data['Class'] == 1]\n",
        "valid = data[data['Class'] == 0]\n",
        "outlierFraction = len(fraud)/float(len(valid))\n",
        "print(outlierFraction)\n",
        "print('Fraud Cases: {}'.format(len(data[data['Class'] == 1])))\n",
        "print('Valid Transactions: {}'.format(len(data[data['Class'] == 0])))"
      ],
      "metadata": {
        "id": "l8KBZwEnxu-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only 0.17% fraudulent transaction out all the transactions. The data is highly Unbalanced. Lets first apply our models without balancing it and if we don’t get a good accuracy then we can find a way to balance this dataset. But first, let’s implement the model without it and will balance the data only if needed.\n",
        "\n",
        "Code : Print the amount details for Fraudulent Transaction"
      ],
      "metadata": {
        "id": "0NH9CeG1xzBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(“Amount details of the fraudulent transaction”)\n",
        "fraud.Amount.describe()"
      ],
      "metadata": {
        "id": "DUhAs0Pzxx6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Print the amount details for Normal Transaction"
      ],
      "metadata": {
        "id": "sgg4Bvcfx3S6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can clearly notice from this, the average Money transaction for the fraudulent ones is more. This makes this problem crucial to deal with.\n"
      ],
      "metadata": {
        "id": "AVPRVjCSx66w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Separating the X and the Y values\n",
        "Dividing the data into inputs parameters and outputs value format"
      ],
      "metadata": {
        "id": "fLnuNJTRyRq0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(['Class'], axis = 1)\n",
        "Y = data[\"Class\"]\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "# getting just the values for the sake of processing\n",
        "# (its a numpy array with no columns)\n",
        "xData = X.values\n",
        "yData = Y.values"
      ],
      "metadata": {
        "id": "lD3qhibfx2_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Testing Data Bifurcation\n",
        "We will be dividing the dataset into two main groups. One for training the model and the other for Testing our trained model’s performance."
      ],
      "metadata": {
        "id": "V4PCBlYjyIAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Split the data into training and testing sets\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(\n",
        "        xData, yData, test_size = 0.2, random_state = 42)"
      ],
      "metadata": {
        "id": "SFfpD6CUyHkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Building a Random Forest Model using skicit learn"
      ],
      "metadata": {
        "id": "jGHQmPO-yZE7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building the Random Forest Classifier (RANDOM FOREST)\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# random forest model creation\n",
        "rfc = RandomForestClassifier()\n",
        "rfc.fit(xTrain, yTrain)\n",
        "# predictions\n",
        "yPred = rfc.predict(xTest)"
      ],
      "metadata": {
        "id": "wszR5r0DyYoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code : Building all kinds of evaluating parameters"
      ],
      "metadata": {
        "id": "scCUayaAyeJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scoring in anything\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import f1_score, matthews_corrcoef\n",
        "from sklearn.metrics import confusion_matrix\n",
        " \n",
        "n_outliers = len(fraud)\n",
        "n_errors = (yPred != yTest).sum()\n",
        "print(\"The model used is Random Forest classifier\")\n",
        " \n",
        "acc = accuracy_score(yTest, yPred)\n",
        "print(\"The accuracy is {}\".format(acc))\n",
        " \n",
        "prec = precision_score(yTest, yPred)\n",
        "print(\"The precision is {}\".format(prec))\n",
        " \n",
        "rec = recall_score(yTest, yPred)\n",
        "print(\"The recall is {}\".format(rec))\n",
        " \n",
        "f1 = f1_score(yTest, yPred)\n",
        "print(\"The F1-Score is {}\".format(f1))"
      ],
      "metadata": {
        "id": "6uV0D6SbygbP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "am4pAsgNyk2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "AZh4jzB6yd7T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}